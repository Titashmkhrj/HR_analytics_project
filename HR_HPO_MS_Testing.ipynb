{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HR_HPO_MS_Testing.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1u42dO-VlwR7ya7Z-rPyhO-XXNsMuCAqC",
      "authorship_tag": "ABX9TyNCT2idpwecm2A0slXqDDI1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Titashmkhrj/HR_analytics_project/blob/master/HR_HPO_MS_Testing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EOuux_xk1tig",
        "colab_type": "text"
      },
      "source": [
        "# Importing libraries."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_5bZFdP-wj2f",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c396d4ca-d0b9-4d2f-b7c0-d308f69cbfea"
      },
      "source": [
        "# import warnings filter\n",
        "from warnings import simplefilter\n",
        "# ignore all future warnings\n",
        "simplefilter(action='ignore', category=FutureWarning)\n",
        "#---------------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "# importing the required libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import joblib\n",
        "\n",
        "import imblearn \n",
        "from imblearn.over_sampling import SMOTENC\n",
        "from imblearn.pipeline import Pipeline\n",
        "\n",
        "from sklearn.linear_model import (LogisticRegression, PassiveAggressiveClassifier, RidgeClassifier)\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import (RandomizedSearchCV, train_test_split, cross_val_score)\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "print(\"Finished importing the libraries.\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Finished importing the libraries.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0d9n0Z1M2E66",
        "colab_type": "text"
      },
      "source": [
        "# Models objects and their parameter grid."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lc3NwALDczVi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# models as per the sequence in the parameter grid \n",
        "model_objects = [LogisticRegression(),\n",
        "                 LogisticRegression(),\n",
        "\t\t\t\t\t\t\t\t LogisticRegression(),\n",
        "\t\t\t\t\t\t\t\t PassiveAggressiveClassifier(),\n",
        "\t\t\t\t\t\t\t\t RidgeClassifier(),\n",
        "\t\t\t\t\t\t\t\t KNeighborsClassifier(),\n",
        "\t\t\t\t\t\t\t\t SVC(),\n",
        "\t\t\t\t\t\t\t\t DecisionTreeClassifier(),\n",
        "\t\t\t\t\t\t\t\t RandomForestClassifier()]\n",
        "\n",
        "\n",
        "\n",
        "# hyper-parameter dictionary for the tunningof the models\n",
        "parameter_grid = {'LR_l1' : {'model__penalty' : ['l1'],\n",
        "                              'model__C' : [0.001, 0.01, 0.1, 1, 10, 100],\n",
        "                              'model__random_state' : [42],\n",
        "                              'model__solver' : ['liblinear', 'saga'],\n",
        "                              'model__max_iter' : [100000]\n",
        "                          },\n",
        "\t\t\t\t\n",
        "                  'LR_l2' : {'model__penalty' : ['l2'],\n",
        "                              'model__C' : [0.001, 0.01, 0.1, 1, 10, 100],\n",
        "                              'model__random_state' : [42],\n",
        "                              'model__solver' : ['newton-cg', 'lbfgs', 'sag', 'saga'],\n",
        "                              'model__max_iter' : [100000]\n",
        "                          },\n",
        "\n",
        "                  'LR_ElNet' : {'model__penalty' : ['elasticnet'],\n",
        "                                'model__l1_ratio' : [0.3, 0.5, 0.7],\n",
        "                                'model__C' : [0.001, 0.01, 0.1, 1, 10, 100],\n",
        "                                'model__random_state' : [42],\n",
        "                                'model__solver' : ['saga'],\n",
        "                                'model__max_iter' : [100000]\n",
        "                              },\n",
        "\n",
        "                  'Pass_Agg_clif' : {'model__C' : [0.001, 0.01, 0.1, 1, 10, 100],\n",
        "                                    #   'model__fit_intercept' : ['True', 'False'],\n",
        "                                      'model__random_state' : [42],\n",
        "                                      'model__loss' : ['hinge', 'squared_hinge'],\n",
        "                                      'model__class_weight' : ['balanced', None]\n",
        "                                  },\n",
        "                  \n",
        "                  'Ridge_clif' : {'model__alpha' : [500.0, 50.0, 5.0, 0.5, 0.05, 0.005],\n",
        "                                  'model__fit_intercept' : ['True', 'False'],\n",
        "                                  'model__normalize' : ['True', 'False'],\n",
        "                                  'model__class_weight' : ['balanced', None],\n",
        "                                  'model__solver' : ['svd', 'cholesky', 'lsqr', 'sparse_cg']\n",
        "                              },\n",
        "                  \n",
        "                  'KN_classif' : {'model__n_neighbors' : [1,3,5,7,9],\n",
        "                                  'model__p' : [1,2,5]                     \n",
        "                              },\n",
        "                  \n",
        "                  'SVC' : {'model__C' : [0.001, 0.01, 0.1, 1, 10, 100, 1000],\n",
        "                           'model__gamma' : ['scale', 'auto'],                     \n",
        "                      },\n",
        "                  \n",
        "                  'DT_clif' : {'model__criterion': ['gini','entropy'],\n",
        "                                'model__max_features': ['sqrt','log2',None],\n",
        "                                'model__min_samples_leaf': [1,2,5,10],\n",
        "                                'model__min_samples_split' : [2,5,10,15,100],\n",
        "                                'model__max_depth': [5,8,15,25,30,None]\n",
        "                          },\n",
        "                  \n",
        "                  'RF_clif' : {'model__n_estimators' : [120,300,500,800,1200],\n",
        "                               'model__max_features': ['sqrt','log2',None],\n",
        "                                'model__min_samples_leaf': [1,2,5,10],\n",
        "                                'model__min_samples_split' : [2,5,10,15,100],\n",
        "                                'model__max_depth': [5,8,15,25,30,None]                      \n",
        "                          }\n",
        "              }"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16aouJHM2zOR",
        "colab_type": "text"
      },
      "source": [
        " # Loading the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GP7oMbnCxaHp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7a1667c3-da7f-4c67-ecf4-b48860bccb6f"
      },
      "source": [
        "# reading the feature and target spaces for our project\n",
        "x_data = pd.read_csv('/content/drive/My Drive/data_for_HPO&MS/HR_attrition/feature_space.csv')\n",
        "y_data = pd.read_csv('/content/drive/My Drive/data_for_HPO&MS/HR_attrition/target_space.csv')\n",
        "\n",
        "# dropping an unnecessary column from our feature and target space\n",
        "if 'EmployeeID' in x_data.columns: \n",
        "    x_data.drop('EmployeeID', axis=1, inplace=True)\n",
        "if 'EmployeeID' in y_data.columns: \n",
        "    y_data.drop('EmployeeID', axis=1, inplace=True)\n",
        "print(\"Finished loading the data.\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Finished loading the data.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9vWhQSIExii9",
        "colab_type": "text"
      },
      "source": [
        "# Splitting the data for the purpose of hyper-parameter optimisation and model selection."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ewZf15Hz1iR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3372e3f7-aa31-48bb-d56c-2d744cd8b682"
      },
      "source": [
        "# splitting our dataset into train, validation and test sets\n",
        "x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size = 0.3, random_state = 42)\n",
        "x_optimization, x_validation, y_optimization, y_validation = train_test_split(x_train, y_train, test_size = 0.3, random_state = 42)\n",
        "\n",
        "print(\"Finished splitting the data.\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Finished splitting the data.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sea9nOWIz5f3",
        "colab_type": "text"
      },
      "source": [
        "# Hyper-parameter optimisation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ykKLF40Mzqt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b4c9c504-5c40-4a89-d22a-7aaecf058edb"
      },
      "source": [
        "# initiating an empty list for storing the optimized models\n",
        "hyper_parameter_optimized_models = []\n",
        "\n",
        "# making the objects for our resampling and scalling the data.\n",
        "'''\n",
        "resampling our optimization datasets, as it is an imbalance dataset,\n",
        "if there are categrical features present in the data, then SMOTENC is normally used, but in our data all the features are catgorical,\n",
        "and SMOTENC cannot operate on only categorical data, so for that reason we will be adding a synthetic nominal feature in our data for the resampling,\n",
        "and then we shall drop the synthethic feature once the data is resampled.\n",
        "'''\n",
        "x_optimization['synthetic_feature'] = np.random.randint(0,100, size=len(x_optimization))\n",
        "'''\n",
        "now for using the SMOTENC we have to state the column index of the categorical features to the resamler algorithm\n",
        "in this case the synthetic feature get added to the dataframe as the last column\n",
        "so it is easy to figure out the column indices of the categorical features\n",
        "'''\n",
        "over_sampler = SMOTENC(categorical_features = np.r_[0:(len(x_optimization.columns)-1)], random_state=56)\n",
        "x_optimization_resampled, y_optimization_resampled = over_sampler.fit_resample(x_optimization, y_optimization)\n",
        "# dropping the sythetic feature after resampling is done\n",
        "y_optimization_resampled = pd.DataFrame(y_optimization_resampled, columns = y_optimization.columns)\n",
        "x_optimization_resampled = pd.DataFrame(x_optimization_resampled, columns = x_optimization.columns)\n",
        "x_optimization_resampled.drop('synthetic_feature', axis=1, inplace=True)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# initiating the random search\n",
        "for grid, model in zip(parameter_grid.values(), model_objects) :\n",
        "  # the only change that i have done is remove the comma \",\" from the end of the very next line i.e classif_model = ......\n",
        "  classif_model = Pipeline([('scaler', scaler), ('model', model)])\n",
        "  # the nex thing tht we can do is remove the over_sampler an scaler objects and define them in te pipeline itself\n",
        "  optimizer = RandomizedSearchCV(estimator = classif_model,\n",
        "\t\t\t\t\t\t\t\tparam_distributions = grid,\n",
        "\t\t\t\t\t\t\t\trandom_state = 42,\n",
        "\t\t\t\t\t\t\t\tcv = 3,\n",
        "\t\t\t\t\t\t\t\terror_score = -1,\n",
        "\t\t\t\t\t\t\t\tverbose = 10,\n",
        "\t\t\t\t\t\t\t\tn_jobs = -1,\n",
        "\t\t\t\t\t\t\t\t)\n",
        "  optimizer.fit(x_optimization_resampled, y_optimization_resampled.values.ravel())\n",
        "\t# appending the best estimator to a list\n",
        "  hyper_parameter_optimized_models.append(optimizer.best_estimator_)\n",
        "\n",
        "print('Hyper parameter tunning is finished.')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  # This is added back by InteractiveShellApp.init_path()\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:    2.9s\n",
            "[Parallel(n_jobs=-1)]: Done   4 tasks      | elapsed:    6.9s\n",
            "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    8.1s\n",
            "[Parallel(n_jobs=-1)]: Done  14 tasks      | elapsed:    9.6s\n",
            "[Parallel(n_jobs=-1)]: Done  21 tasks      | elapsed:    9.9s\n",
            "[Parallel(n_jobs=-1)]: Done  30 out of  30 | elapsed:   18.2s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:    0.1s\n",
            "[Parallel(n_jobs=-1)]: Batch computation too fast (0.0809s.) Setting batch_size=2.\n",
            "[Parallel(n_jobs=-1)]: Done   4 tasks      | elapsed:    0.2s\n",
            "[Parallel(n_jobs=-1)]: Done  14 tasks      | elapsed:    1.5s\n",
            "[Parallel(n_jobs=-1)]: Done  24 tasks      | elapsed:    1.8s\n",
            "[Parallel(n_jobs=-1)]: Done  30 out of  30 | elapsed:    2.2s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:    0.1s\n",
            "[Parallel(n_jobs=-1)]: Batch computation too fast (0.0772s.) Setting batch_size=2.\n",
            "[Parallel(n_jobs=-1)]: Done   4 tasks      | elapsed:    0.2s\n",
            "[Parallel(n_jobs=-1)]: Batch computation too fast (0.1982s.) Setting batch_size=4.\n",
            "[Parallel(n_jobs=-1)]: Done  16 tasks      | elapsed:    2.9s\n",
            "[Parallel(n_jobs=-1)]: Batch computation too slow (2.6643s.) Setting batch_size=1.\n",
            "[Parallel(n_jobs=-1)]: Done  30 out of  30 | elapsed:   16.6s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:    0.0s\n",
            "[Parallel(n_jobs=-1)]: Batch computation too fast (0.0365s.) Setting batch_size=2.\n",
            "[Parallel(n_jobs=-1)]: Done   4 tasks      | elapsed:    0.1s\n",
            "[Parallel(n_jobs=-1)]: Batch computation too fast (0.0927s.) Setting batch_size=4.\n",
            "[Parallel(n_jobs=-1)]: Done  16 tasks      | elapsed:    0.3s\n",
            "[Parallel(n_jobs=-1)]: Batch computation too fast (0.1764s.) Setting batch_size=8.\n",
            "[Parallel(n_jobs=-1)]: Done  30 out of  30 | elapsed:    0.5s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:    0.1s\n",
            "[Parallel(n_jobs=-1)]: Batch computation too fast (0.0565s.) Setting batch_size=2.\n",
            "[Parallel(n_jobs=-1)]: Done   4 tasks      | elapsed:    0.1s\n",
            "[Parallel(n_jobs=-1)]: Batch computation too fast (0.1193s.) Setting batch_size=4.\n",
            "[Parallel(n_jobs=-1)]: Done  16 tasks      | elapsed:    0.4s\n",
            "[Parallel(n_jobs=-1)]: Done  30 out of  30 | elapsed:    0.6s finished\n",
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:    0.6s\n",
            "[Parallel(n_jobs=-1)]: Done   4 tasks      | elapsed:    6.8s\n",
            "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    9.7s\n",
            "[Parallel(n_jobs=-1)]: Done  14 tasks      | elapsed:   20.4s\n",
            "[Parallel(n_jobs=-1)]: Done  21 tasks      | elapsed:   39.9s\n",
            "[Parallel(n_jobs=-1)]: Done  30 out of  30 | elapsed:   52.6s finished\n",
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:    0.3s\n",
            "[Parallel(n_jobs=-1)]: Done   4 tasks      | elapsed:    0.9s\n",
            "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    3.0s\n",
            "[Parallel(n_jobs=-1)]: Done  14 tasks      | elapsed:    4.8s\n",
            "[Parallel(n_jobs=-1)]: Done  21 tasks      | elapsed:    7.8s\n",
            "[Parallel(n_jobs=-1)]: Done  30 out of  30 | elapsed:   10.8s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:    0.1s\n",
            "[Parallel(n_jobs=-1)]: Batch computation too fast (0.0574s.) Setting batch_size=2.\n",
            "[Parallel(n_jobs=-1)]: Done   4 tasks      | elapsed:    0.1s\n",
            "[Parallel(n_jobs=-1)]: Batch computation too fast (0.1395s.) Setting batch_size=4.\n",
            "[Parallel(n_jobs=-1)]: Done  16 tasks      | elapsed:    0.4s\n",
            "[Parallel(n_jobs=-1)]: Batch computation too fast (0.1598s.) Setting batch_size=8.\n",
            "[Parallel(n_jobs=-1)]: Done  30 out of  30 | elapsed:    0.5s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:    4.5s\n",
            "[Parallel(n_jobs=-1)]: Done   4 tasks      | elapsed:   20.9s\n",
            "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:   35.9s\n",
            "[Parallel(n_jobs=-1)]: Done  14 tasks      | elapsed:   44.9s\n",
            "[Parallel(n_jobs=-1)]: Done  21 tasks      | elapsed:  1.1min\n",
            "[Parallel(n_jobs=-1)]: Done  30 out of  30 | elapsed:  1.4min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Hyper parameter tunning is finished.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7qBWRc0-0Fww",
        "colab_type": "text"
      },
      "source": [
        "# Model selection."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qb5WNBI40GR_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        },
        "outputId": "dbfa18e3-1087-44c3-eda5-fab15394836b"
      },
      "source": [
        "# initiating an empty list to stre the validation scores of the optimized models\n",
        "optimized_model_validation_scores = []\n",
        "\n",
        "x_validation['synthetic_feature'] = np.random.randint(0,100, size=len(x_validation))\n",
        "resampler = SMOTENC(categorical_features = np.r_[0:(len(x_validation.columns)-1)], random_state=56)\n",
        "x_validation_resampled, y_validation_resampled = resampler.fit_resample(x_validation, y_validation)\n",
        "# dropping the sythetic feature after resampling is done\n",
        "y_validation_resampled = pd.DataFrame(y_validation_resampled, columns = y_validation.columns)\n",
        "x_validation_resampled = pd.DataFrame(x_validation_resampled, columns = x_validation.columns)\n",
        "x_validation_resampled.drop('synthetic_feature', axis=1, inplace=True)\n",
        "\n",
        "\n",
        "for optimized_model in hyper_parameter_optimized_models :\n",
        "  optimized_model_pipeline = Pipeline([('scaler', scaler), ('optimized_model', optimized_model)])\n",
        "  model_validation_scores = cross_val_score(optimized_model_pipeline, x_validation_resampled, y_validation_resampled.values.ravel(), cv=3, n_jobs = -1)\n",
        "  optimized_model_validation_scores.append(np.mean(model_validation_scores))\n",
        "\n",
        "# making a dictionary to store the results of the hyper-parameter optimization and the model selection process.\n",
        "results_dict = {'optimized_model':hyper_parameter_optimized_models,\n",
        "                'validation_score':optimized_model_validation_scores\n",
        "                }\n",
        "\n",
        "optimized_model_results = pd.DataFrame(results_dict)\n",
        "# saving the results of the hyper-parameter optimization and model_selection in a csv file\n",
        "optimized_model_results.to_csv('/content/drive/My Drive/data_for_HPO&MS/HR_attrition/model_optimizaion_report.csv')\n",
        "print('Model selection is finished')\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  after removing the cwd from sys.path.\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model selection is finished\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_uStoB60HJe",
        "colab_type": "text"
      },
      "source": [
        "# Best performing hyper-parameter optimised model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "olBhpvwX0Haj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "e42f6c52-dbad-4feb-edf6-caf1dfabb5d3"
      },
      "source": [
        "print('Initiating the process of our final phase to judge the average out-of-sample performance of our best found optimized model.')\n",
        "# selecting the best model by its index for the final predictions\n",
        "best_model_idx = optimized_model_results['validation_score'].idxmax(axis=0)\n",
        "best_model = optimized_model_results.iloc[best_model_idx,0]\n",
        "\n",
        "print('The best model to our finding is ', best_model)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Initiating the process of our final phase to judge the average out-of-sample performance of our best found optimized model.\n",
            "The best model to our finding is  Pipeline(memory=None,\n",
            "         steps=[('scaler',\n",
            "                 StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
            "                ('model',\n",
            "                 SVC(C=10, break_ties=False, cache_size=200, class_weight=None,\n",
            "                     coef0=0.0, decision_function_shape='ovr', degree=3,\n",
            "                     gamma='auto', kernel='rbf', max_iter=-1, probability=False,\n",
            "                     random_state=None, shrinking=True, tol=0.001,\n",
            "                     verbose=False))],\n",
            "         verbose=False)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xbD_ZYMI1uuL",
        "colab_type": "text"
      },
      "source": [
        "# Defining the best model from the above findings, that will be futher used for the final prediction making."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RDDwBqYD1tzD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "61560b1f-100d-4728-d366-af1af6242a0b"
      },
      "source": [
        "# selecting the classifier algorithm from the pipeline of the best model found.\n",
        "final_model = best_model[1]\n",
        "final_model"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SVC(C=10, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
              "    decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
              "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
              "    tol=0.001, verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T6vMtqe_0H01",
        "colab_type": "text"
      },
      "source": [
        "# Final Prediction."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rcwIg8x60Huw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        },
        "outputId": "1c771661-e1fb-4052-9e19-77325eab2220"
      },
      "source": [
        "# we are utilizing the whole training dataset for training the fianl model before making predictions on the test set.\n",
        "# resampling our training datasets, in order to prevent overfitting of our models on the majority class of the target feature in our training set\n",
        "x_train['synthetic_feature'] = np.random.randint(0,100, size=len(x_train))\n",
        "resampler = SMOTENC(categorical_features = np.r_[0:(len(x_train.columns)-1)], random_state=56)\n",
        "x_train_resampled, y_train_resampled = resampler.fit_resample(x_train, y_train)\n",
        "# dropping the sythetic feature after resampling is done\n",
        "y_train_resampled = pd.DataFrame(y_train_resampled, columns = y_train.columns)\n",
        "x_train_resampled = pd.DataFrame(x_train_resampled, columns = x_train.columns)\n",
        "x_train_resampled.drop('synthetic_feature', axis=1, inplace=True)\n",
        "\n",
        "# scaling our features in the training dataset\n",
        "scaler = StandardScaler().fit(x_train_resampled)\n",
        "x_train_scaled = scaler.transform(x_train_resampled)\n",
        "x_test_scaled = scaler.transform(x_test)\n",
        "\n",
        "# re-fitting out best found optimized model to the whole training set\n",
        "final_model.fit(x_train_scaled, y_train_resampled.values.ravel())\n",
        "out_of_sample_predictions = final_model.predict(x_test_scaled)\n",
        "\n",
        "final_score = accuracy_score(y_test, out_of_sample_predictions)\n",
        "\n",
        "print('The final average out-of-sample performance score of our best optimized model is', final_score)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "The final average out-of-sample performance score of our best optimized model is 0.976905311778291\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5JFiQ8w50Hl9",
        "colab_type": "text"
      },
      "source": [
        "# Saving the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mlQh0ZAn1pOs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3f5f349f-8314-431e-8c3f-de2c3db4ae9a"
      },
      "source": [
        "# saving our best found optimized model for this data, as a pickle file\n",
        "joblib.dump(best_model, '/content/drive/My Drive/data_for_HPO&MS/HR_attrition/best_model.pkl')\n",
        "joblib.dump(final_model, '/content/drive/My Drive/data_for_HPO&MS/HR_attrition/final_model.pkl')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/drive/My Drive/data_for_HPO&MS/HR_attrition/final_model.pkl']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    }
  ]
}